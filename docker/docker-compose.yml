version: '3.8'

services:
  # =============================================================================
  # Application Services
  # =============================================================================
  backend:
    build:
      context: ../backend
      dockerfile: Dockerfile
    container_name: graphrag-vllm-backend
    ports:
      - "18000:8000"
    environment:
      - DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=${NEO4J_PASSWORD}
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - LLM_BACKEND=${LLM_BACKEND:-vllm}
      - VLLM_BASE_URL=${VLLM_BASE_URL:-http://vllm:8000/v1}
      - VLLM_MODEL=${VLLM_MODEL:-spow12/Ko-Qwen2-7B-Instruct}
      - VLLM_EMBEDDING_BASE_URL=${VLLM_EMBEDDING_BASE_URL:-http://vllm-embedding:8000/v1}
      - VLLM_EMBEDDING_MODEL=${VLLM_EMBEDDING_MODEL:-upskyy/bge-m3-korean}
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
      - JWT_ALGORITHM=HS256
      - JWT_EXPIRE_MINUTES=60
      - ADMIN_EMAIL=${ADMIN_EMAIL}
      - ADMIN_PASSWORD=${ADMIN_PASSWORD}
    volumes:
      - pdf_storage:/app/storage
      - ../backend/src:/app/src:ro
    depends_on:
      postgres:
        condition: service_healthy
      neo4j:
        condition: service_healthy
      qdrant:
        condition: service_started
      redis:
        condition: service_healthy
    networks:
      - graphrag-vllm-network
    restart: unless-stopped

  celery-worker:
    build:
      context: ../backend
      dockerfile: Dockerfile
    container_name: graphrag-vllm-celery-worker
    command: celery -A src.core.celery_app worker --loglevel=info --concurrency=3 -Q celery,documents,stats
    environment:
      - DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=${NEO4J_PASSWORD}
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - LLM_BACKEND=${LLM_BACKEND:-vllm}
      - VLLM_BASE_URL=${VLLM_BASE_URL:-http://vllm:8000/v1}
      - VLLM_MODEL=${VLLM_MODEL:-spow12/Ko-Qwen2-7B-Instruct}
      - VLLM_EMBEDDING_BASE_URL=${VLLM_EMBEDDING_BASE_URL:-http://vllm-embedding:8000/v1}
      - VLLM_EMBEDDING_MODEL=${VLLM_EMBEDDING_MODEL:-upskyy/bge-m3-korean}
    volumes:
      - pdf_storage:/app/storage
      - ../backend/src:/app/src:ro
    depends_on:
      - redis
      - postgres
      - neo4j
      - qdrant
    deploy:
      resources:
        limits:
          memory: 4G
    healthcheck:
      test: ["CMD-SHELL", "celery -A src.core.celery_app inspect ping -d celery@$$HOSTNAME || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - graphrag-vllm-network
    restart: unless-stopped

  celery-beat:
    build:
      context: ../backend
      dockerfile: Dockerfile
    container_name: graphrag-vllm-celery-beat
    command: celery -A src.core.celery_app beat --loglevel=info
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
    depends_on:
      - redis
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import redis; r = redis.Redis(host='redis', port=6379); exit(0 if r.ping() else 1)\""]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - graphrag-vllm-network
    restart: unless-stopped

  flower:
    image: mher/flower:2.0
    container_name: graphrag-vllm-flower
    command: celery --broker=redis://redis:6379/0 flower --port=5555
    ports:
      - "15555:5555"
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
    depends_on:
      - redis
    networks:
      - graphrag-vllm-network
    restart: unless-stopped

  frontend:
    build:
      context: ../frontend
      dockerfile: Dockerfile
    container_name: graphrag-vllm-frontend
    ports:
      - "13000:3000"
    environment:
      - VITE_API_URL=http://localhost:18000/api/v1
    depends_on:
      - backend
    networks:
      - graphrag-vllm-network
    restart: unless-stopped

  # =============================================================================
  # Database Services
  # =============================================================================
  postgres:
    image: postgres:15-alpine
    container_name: graphrag-vllm-postgres
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    ports:
      - "15432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ../databases/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - graphrag-vllm-network
    restart: unless-stopped

  neo4j:
    image: neo4j:5.15-community
    container_name: graphrag-vllm-neo4j
    environment:
      - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD}
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_apoc_export_file_enabled=true
      - NEO4J_apoc_import_file_enabled=true
      - NEO4J_apoc_import_file_use__neo4j__config=true
    ports:
      - "17474:7474"  # HTTP
      - "17687:7687"  # Bolt
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - ../databases/neo4j:/var/lib/neo4j/import
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:7474 || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - graphrag-vllm-network
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:latest
    container_name: graphrag-vllm-qdrant
    ports:
      - "16333:6333"  # REST API
      - "16334:6334"  # gRPC
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - graphrag-vllm-network
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: graphrag-vllm-redis
    ports:
      - "16379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - graphrag-vllm-network
    restart: unless-stopped

  # =============================================================================
  # vLLM Services (LLM + Embedding)
  # =============================================================================

  # vLLM LLM Server (Chat/Text Generation)
  vllm:
    image: vllm/vllm-openai:v0.6.6.post1
    container_name: graphrag-vllm-llm
    ports:
      - "18001:8000"
      - "18002:8001"  # Prometheus metrics port
    volumes:
      - vllm_cache:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
      - VLLM_USAGE_STATS=0
      - PYTHONUNBUFFERED=1
    command: >
      --model ${VLLM_MODEL:-spow12/Ko-Qwen2-7B-Instruct}
      --gpu-memory-utilization 0.75
      --max-model-len 4096
      --enable-chunked-prefill
      --trust-remote-code
      --disable-log-requests
      --enforce-eager
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    networks:
      - graphrag-vllm-network
    restart: unless-stopped

  # vLLM Embedding Server (Vector Embeddings)
  vllm-embedding:
    image: vllm/vllm-openai:v0.6.6.post1
    container_name: graphrag-vllm-embed
    ports:
      - "18003:8000"
      - "18004:8001"  # Prometheus metrics port
    volumes:
      - vllm_embedding_cache:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
      - VLLM_USAGE_STATS=0
      - PYTHONUNBUFFERED=1
      - VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
    command: >
      --model ${VLLM_EMBEDDING_MODEL:-upskyy/bge-m3-korean}
      --gpu-memory-utilization 0.15
      --max-model-len 1024
      --trust-remote-code
      --disable-log-requests
      --enforce-eager
      --task embed
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    networks:
      - graphrag-vllm-network
    restart: unless-stopped

# =============================================================================
# Networks & Volumes
# =============================================================================
networks:
  graphrag-vllm-network:
    driver: bridge

volumes:
  postgres_data:
  neo4j_data:
  neo4j_logs:
  qdrant_data:
  redis_data:
  pdf_storage:
  vllm_cache:
  vllm_embedding_cache:
